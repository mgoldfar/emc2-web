<div class="row">
  <div class="col s12">
    <h3 class="header"><i class="material-icons">emoji_events</i> EMC² Competition: AI Infrastructure Demos</h3>
    <p class="secondary-text" style="margin-top: -0.5em;">Sponsored by <a href="https://runara.ai" target="_blank">Runara.ai</a></p>
  </div>
</div>

<div class="row">
  <div class="col s12">
    <h4 class="header">Overview</h4>
    <p>
      The EMC² Competition invites researchers, practitioners, and students to present production-ready AI infrastructure systems demonstrating measurable improvements in efficiency, observability, and operational robustness. The competition is designed to surface systems that operate under real-world constraints- including scale, reliability, cost, and sustainability.
    </p>
    <p>
      The primary goal is to move beyond toy benchmarks, isolated micro-optimizations, and paper-only abstractions. We want to see working demos, toolkits, and integrated systems that can be realistically deployed in modern AI infrastructure environments. Submissions should emphasize live execution, actionable telemetry, and end-to-end impact rather than synthetic evaluations.
    </p>
  </div>
</div>

<div class="row">
  <div class="col s12">
    <h4 class="header">Competition Tracks</h4>
  </div>
  <div class="col s12 m6">
    <div class="card z-depth-1">
      <div class="card-content">
        <span class="card-title dark-primary"><strong>Track 1: Efficient Inference</strong></span>
        <p>Systems improving cost, performance, and energy efficiency of inference workloads:</p>
        <ul class="browser-default">
          <li>Runtime inference optimization</li>
          <li>Scheduling, batching, parallelism</li>
          <li>Hardware-aware execution</li>
          <li>Cost/energy-aware pipelines</li>
        </ul>
      </div>
    </div>
  </div>
  <div class="col s12 m6">
    <div class="card z-depth-1">
      <div class="card-content">
        <span class="card-title dark-primary"><strong>Track 2: Infrastructure Observability</strong></span>
        <p>Visibility, monitoring, and diagnostics across the AI stack:</p>
        <ul class="browser-default">
          <li>Live telemetry and visualization</li>
          <li>Utilization and bottleneck analysis</li>
          <li>Cross-layer observability</li>
          <li>Actionable operator insights</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<div class="row">
  <div class="col s12">
    <h4 class="header">Hardware and Model Flexibility</h4>
    <p>
      Participants are free to choose any hardware platform and any inference model, including GPUs or custom accelerators, cloud-based, on-premise, or edge environments, and open-source or proprietary models. There are no restrictions on vendors, architectures, or model families, provided the submission demonstrates real execution, live metrics, and measurable impact.
    </p>
  </div>
</div>

<div class="row">
  <div class="col s12">
    <h4 class="header">Submission Requirements</h4>
    <ul class="browser-default">
      <li><strong>Team size:</strong> Up to 2 participants</li>
      <li><strong>Format:</strong> Single-page proposal (PDF) describing the problem, system architecture, hardware/model used, live metrics captured, and how efficiency or observability improvements are demonstrated</li>
      <li><strong>Originality:</strong> The idea does not need to be novel, but implementation must be original. Prior work may be extended with clear attribution.</li>
      <li><strong>Demo requirement (mandatory):</strong> Submissions must include a working demo that tracks live metrics, demonstrates real execution, and clearly shows improvements in cost, performance, or sustainability. Simulation-only or slide-only submissions will not be considered.</li>
    </ul>
    <p><strong>Submission deadline: March 15</strong></p>
  </div>
</div>

<div class="row">
  <div class="col s12">
    <h4 class="header">Selection &amp; Presentation</h4>
    <p>
      Accepted teams will display their demos during the workshop lunch break and engage directly with judges during in-person evaluations. Based on judging scores, the top 3 teams from each track will be selected for oral presentations.
    </p>
    <p>
      <strong>Final Presentations:</strong> 4:00 - 5:00 PM, 10 minutes per team, Live demo encouraged
    </p>
    <p>
      At the conclusion of the session, one winning team per track will be selected.
    </p>
  </div>
</div>

<div class="row">
  <div class="col s12">
    <h4 class="header">Awards</h4>
    <div class="card z-depth-1">
      <div class="card-content center-align">
        <p style="font-size: 1.1em;">Each winning team receives:</p>
        <h5 class="dark-primary" style="margin: 0.5em 0;"><strong>$500 cash prize</strong> + Certificate of Recognition</h5>
        <p>Winners eligible for <strong>Summer Internship opportunities at Runara.ai</strong></p>
      </div>
    </div>
  </div>
</div>

<div class="row">
  <div class="col s12">
    <h4 class="header">Evaluation Criteria</h4>
    <p>
      Since this is an open-scope, applied competition, proposals will be evaluated based on their real-world application potential instead of rigid benchmark metrics. Judging prioritizes: production readiness and robustness, clarity and usefulness of metrics, real measurable end-to-end impact, clean system design, and practical relevance to modern AI infrastructure.
    </p>
  </div>
</div>

<div class="row">
  <div class="col s12">
    <p>For any questions please contact <a href="mailto:raj@runara.ai">raj@runara.ai</a></p>
  </div>
</div>
