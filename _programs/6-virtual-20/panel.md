---
edition: virtual-20
type: panel
time_start: 2020-12-05 16:40:00
time_end: 2020-12-05 17:40:00
title: "Towards Million-fold Efficiency in AI"
moderator:
    name: Satyam Srivastava
    affiliation: Intel Corporation
speakers: Vikram Saletore
 
---
  
Training the largest known transformer models consume megawatts of power. Yet, we are far less capable compared to the human brain. In this panel, we will ask what will it take to train models that understand natural languages like our brain can. Would we continue to increase the energy footprint or innovate and find efficient solutions that consume far less and still can reach the illustrious magnanimty of the human brain?
 
