<div class="row">
  <div class="col s12">
      <h3 class="header"><i class="material-icons">description</i> Workshop Objective</h3>
      <p>
        Transformers are the foundational principles of large deep learning language models. Recent successes of Transformer-based models in image classification and action prediction use cases indicate their wide applicability. In this workshop, we want to focus on the leading ideas using Transformer models such as PALM from Google. We will learn what have been their key observations on performance of the model, optimizations for inference and power consumption of both mixed-precision inference and training.</p>
  </div>
</div>
<div class="row">
  {% assign _col_size="s12 m6" %}
  {% if include.hide_cfp %}
    {% assign _col_size="s12" %}
  {% else %}
  <div class="col {{ _col_size }}">
    <h3 class="header"><i class="material-icons">chat</i> Call for Papers</h3>
    <p>
    The goal of this Workshop is to provide a forum for researchers and industry experts who are exploring novel ideas, tools, and techniques to improve the energy efficiency of machine learning and deep learning as it is practiced today and would evolve in the next decade. We envision that only through close collaboration between industry and the academia we will be able to address the difficult challenges and opportunities of reducing the carbon footprint of AI and its uses. We have tailored our program to best serve the participants in a fully digital setting.  Our forum facilitates active exchange of ideas through: </p>

    <ul class="browser-default list">

	    <li>Keynotes, invited talks and discussion panels by leading researchers from industry and academia</li>
	    <li>Peer-reviewed papers on latest solutions including works-in-progress to seek directed feedback from experts</li>
	    <li>Independent publication of proceedings through IEEE CPS</li>

    </ul>
    <p>
    We invite full-length papers describing original, cutting-edge, and even work-in-progress research projects about efficient machine learning. Suggested topics for papers include, but are not limited to the ones listed on this page. The proceedings from previous instances have been published through the prestigious IEEE Conference Publishing Services (CPS) and are available to the community via IEEE Xplore. In each instance, IEEE conducted independent assessment of the papers for quality.
    </p>

  </div>
  {% endif %}
  <div class="col {{ _col_size }}">
    <h3 class="header"><i class="material-icons">format_list_bulleted</i> Topics for the Workshop</h3>
    <ul class="browser-default list">
	    <li>Neural network architectures for resource constrained applications</li>
	    <li>Efficient hardware designs to implement neural networks including sparsity, locality, and systolic designs</li>
	    <li>Power and performance efficient memory architectures suited for neural networks</li>
	    <li>Network reduction techniques â€“ approximation, quantization, reduced precision, pruning, distillation, and reconfiguration</li>
	    <li>Exploring interplay of precision, performance, power, and energy through benchmarks, workloads, and characterization</li>
	    <li>Simulation and emulation techniques, frameworks, tools, and platforms for machine learning</li>
	    <li>Optimizations to improve performance of training techniques including on-device and large-scale learning</li>
	    <li>Load balancing and efficient task distribution, communication and computation overlapping for optimal performance</li>
	    <li>Verification, validation, determinism, robustness, bias, safety, and privacy challenges in AI systems</li>
    </ul>
  </div>
</div>
